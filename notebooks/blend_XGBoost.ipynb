{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Training an XGBoost model for [MoA prediction on kaggle](https://www.kaggle.com/c/lish-moa/overview) \n\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/lish-moa/sample_submission.csv\n/kaggle/input/lish-moa/train_targets_scored.csv\n/kaggle/input/lish-moa/train_targets_nonscored.csv\n/kaggle/input/lish-moa/train_features.csv\n/kaggle/input/lish-moa/test_features.csv\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Importing the good stuff\n\n* We'll be training an XGBoost model here, but since it's a multilabel problem, we'll use the `MultiOutputClassifier` as a wrapper over the `XGBClassifier` \n"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import KFold\nfrom category_encoders import CountEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import log_loss\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.multioutput import MultiOutputClassifier\n\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Some important params\n\n* You could go for a larger number or folds, but that would take much longer to train, and won't necessarily give better results for this dataset \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"SEED = 42\nNFOLDS = 5\nDATA_DIR = '/kaggle/input/lish-moa/'\nnp.random.seed(SEED)","execution_count":3,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading all the CSV files"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(DATA_DIR + 'train_features.csv')\ntargets = pd.read_csv(DATA_DIR + 'train_targets_scored.csv')\n\ntest = pd.read_csv(DATA_DIR + 'test_features.csv')\nsub = pd.read_csv(DATA_DIR + 'sample_submission.csv')","execution_count":4,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dropping the `sig_id` column for training "},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train.iloc[:,1:].to_numpy()\nX_test = test.iloc[:,1:].to_numpy()\ny = targets.iloc[:,1:].to_numpy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Defining the pipeline\n\n\n* `MultiOutputClassifier` is built based on the idea of fitting one classifier per target. This is a simple strategy for extending classifiers that do not natively support multi-target classification\n* `tree_method='gpu_hist'` bascally tells XGBoost to utilize a CUDA capable device if available \n* The pipeline has2 main parts:\n\n    * `CountEncoder` is used to encode categorical values where the argument  `cols` specifies the list of columns to encode \n    * `classifier` is the `MultiOutputClassifier` that we just built"},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier = MultiOutputClassifier(XGBClassifier(tree_method='gpu_hist'))\n\nclf = Pipeline([('encode', CountEncoder(cols=[0, 2])),\n                ('classify', classifier)\n               ])","execution_count":5,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Fine tuning \n\n* `classify__estimator__colsample_bytree` specifies the maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit\n* `classify__estimator__learning_rate` the learning rate\n* `classify__estimator__max_delta_step` specifies the maximum step size in each iteration, setting this to `0` means there's no limit. But this constraint helps when training highly imbalanced logistic regression models \n* `classify__estimator__subsample` specifies the subsample ratio of the training instances. Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees."},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'classify__estimator__colsample_bytree': 0.6522,\n          'classify__estimator__gamma': 3.6975,\n          'classify__estimator__learning_rate': 0.0503,\n          'classify__estimator__max_delta_step': 2.0706,\n          'classify__estimator__max_depth': 10,\n          'classify__estimator__min_child_weight': 31.5800,\n          'classify__estimator__n_estimators': 166,\n          'classify__estimator__subsample': 0.8639\n         }\n\n_ = clf.set_params(**params)","execution_count":6,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The training loop \n\nSome notes first:\n* OOF means Out Of Fold, equivalent to a \"holdout set\"\n* K fold cross valudation involves randomly dividing the set of observations into k groups, or folds, of approximately equal size. The first fold is treated as a validation set, and the method is fit on the remaining k − 1 folds.\n\n**important**: we're dropping all the features where `cp_type==ctl_vehicle` because the targets of these rows are always zero, so we hardcode it into the final submission. We're dropping it here\n\n```\nctl_mask = X_train[:,0]=='ctl_vehicle'\nX_train = X_train[~ctl_mask,:]\ny_train = y_train[~ctl_mask]\n```\n\n* We're using `predict_proba` here instead of the usual `predict`. \n    *  `predict` will give you output like `0`,`1`\n    * `predict_proba` will give you the probability value of y being `0` or `1`.\n\nOur final prediction on the test set  is basically an average of the predictions made the the models trained on each fold\n\n**Useful links for a beginner like myself:**\n* [What is log loss ? explained by a kaggle grandmaster](https://www.kaggle.com/dansbecker/what-is-log-loss)\n* [Video explaining log loss](https://www.youtube.com/watch?v=MztgenIfGgM&ab_channel=BhaveshBhatt)"},{"metadata":{"trusted":true},"cell_type":"code","source":"oof_preds = np.zeros(y.shape)\ntest_preds = np.zeros((test.shape[0], y.shape[1]))\noof_losses = []\nkf = KFold(n_splits=NFOLDS)\nfor fn, (trn_idx, val_idx) in enumerate(kf.split(X, y)):\n    print('Starting fold: ', fn)\n    X_train, X_val = X[trn_idx], X[val_idx]\n    y_train, y_val = y[trn_idx], y[val_idx]\n    \n    # drop where cp_type==ctl_vehicle (baseline)\n    ctl_mask = X_train[:,0]=='ctl_vehicle'\n    X_train = X_train[~ctl_mask,:]\n    y_train = y_train[~ctl_mask]\n    \n    clf.fit(X_train, y_train)\n    val_preds = clf.predict_proba(X_val) # list of preds per class\n    val_preds = np.array(val_preds)[:,:,1].T # take the positive class\n    oof_preds[val_idx] = val_preds\n    \n    loss = log_loss(np.ravel(y_val), np.ravel(val_preds))\n    oof_losses.append(loss)\n    preds = clf.predict_proba(X_test)\n    preds = np.array(preds)[:,:,1].T # take the positive class\n    test_preds += preds / NFOLDS  ## good old averaging \n    \nprint(oof_losses)\nprint('Mean OOF loss across folds', np.mean(oof_losses))\nprint('STD OOF loss across folds', np.std(oof_losses))","execution_count":7,"outputs":[{"output_type":"stream","text":"Starting fold:  0\nStarting fold:  1\nStarting fold:  2\nStarting fold:  3\nStarting fold:  4\n[0.0169781773377249, 0.01704491710861325, 0.016865153552168475, 0.01700900926983899, 0.01717882474706338]\nMean OOF loss across folds 0.017015216403081797\nSTD OOF loss across folds 0.00010156682747757948\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Hardcoding the preds where `train['cp_type']=='ctl_vehicle'` on OOF preds and test preds "},{"metadata":{"trusted":true},"cell_type":"code","source":"# set control train preds to 0\ncontrol_mask = train['cp_type']=='ctl_vehicle'\noof_preds[control_mask] = 0\n\nprint('OOF log loss: ', log_loss(np.ravel(y), np.ravel(oof_preds)))","execution_count":8,"outputs":[{"output_type":"stream","text":"OOF log loss:  0.0167240932391125\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"control_mask = test['cp_type']=='ctl_vehicle'\ntest_preds[control_mask] = 0","execution_count":9,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Making a submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.iloc[:,1:] = test_preds\nsub.to_csv('submission.csv', index=False)","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.head()","execution_count":11,"outputs":[{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"         sig_id  5-alpha_reductase_inhibitor  11-beta-hsd1_inhibitor  \\\n0  id_0004d9e33                     0.002041                0.002052   \n1  id_001897cda                     0.002041                0.002052   \n2  id_002429b5b                     0.000000                0.000000   \n3  id_00276f245                     0.002041                0.002052   \n4  id_0027f1083                     0.002041                0.002052   \n\n   acat_inhibitor  acetylcholine_receptor_agonist  \\\n0        0.002055                        0.011048   \n1        0.002055                        0.003816   \n2        0.000000                        0.000000   \n3        0.002055                        0.010955   \n4        0.002055                        0.014007   \n\n   acetylcholine_receptor_antagonist  acetylcholinesterase_inhibitor  \\\n0                           0.013914                        0.003767   \n1                           0.004644                        0.003374   \n2                           0.000000                        0.000000   \n3                           0.008548                        0.003237   \n4                           0.017502                        0.002726   \n\n   adenosine_receptor_agonist  adenosine_receptor_antagonist  \\\n0                    0.002396                       0.005898   \n1                    0.002030                       0.005303   \n2                    0.000000                       0.000000   \n3                    0.002358                       0.004069   \n4                    0.003422                       0.004535   \n\n   adenylyl_cyclase_activator  ...  tropomyosin_receptor_kinase_inhibitor  \\\n0                    0.002035  ...                               0.002038   \n1                    0.002035  ...                               0.002038   \n2                    0.000000  ...                               0.000000   \n3                    0.002035  ...                               0.002038   \n4                    0.002035  ...                               0.002038   \n\n   trpv_agonist  trpv_antagonist  tubulin_inhibitor  \\\n0      0.001974         0.002214           0.003002   \n1      0.002162         0.002117           0.001061   \n2      0.000000         0.000000           0.000000   \n3      0.002048         0.002364           0.005247   \n4      0.001964         0.002480           0.005332   \n\n   tyrosine_kinase_inhibitor  ubiquitin_specific_protease_inhibitor  \\\n0                   0.002947                               0.002033   \n1                   0.005006                               0.002033   \n2                   0.000000                               0.000000   \n3                   0.002792                               0.002033   \n4                   0.002750                               0.002033   \n\n   vegfr_inhibitor  vitamin_b  vitamin_d_receptor_agonist  wnt_inhibitor  \n0         0.002294   0.002079                    0.001879       0.002083  \n1         0.005964   0.002051                    0.002421       0.002085  \n2         0.000000   0.000000                    0.000000       0.000000  \n3         0.002569   0.002070                    0.002198       0.002063  \n4         0.002204   0.002079                    0.001766       0.002074  \n\n[5 rows x 207 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sig_id</th>\n      <th>5-alpha_reductase_inhibitor</th>\n      <th>11-beta-hsd1_inhibitor</th>\n      <th>acat_inhibitor</th>\n      <th>acetylcholine_receptor_agonist</th>\n      <th>acetylcholine_receptor_antagonist</th>\n      <th>acetylcholinesterase_inhibitor</th>\n      <th>adenosine_receptor_agonist</th>\n      <th>adenosine_receptor_antagonist</th>\n      <th>adenylyl_cyclase_activator</th>\n      <th>...</th>\n      <th>tropomyosin_receptor_kinase_inhibitor</th>\n      <th>trpv_agonist</th>\n      <th>trpv_antagonist</th>\n      <th>tubulin_inhibitor</th>\n      <th>tyrosine_kinase_inhibitor</th>\n      <th>ubiquitin_specific_protease_inhibitor</th>\n      <th>vegfr_inhibitor</th>\n      <th>vitamin_b</th>\n      <th>vitamin_d_receptor_agonist</th>\n      <th>wnt_inhibitor</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>id_0004d9e33</td>\n      <td>0.002041</td>\n      <td>0.002052</td>\n      <td>0.002055</td>\n      <td>0.011048</td>\n      <td>0.013914</td>\n      <td>0.003767</td>\n      <td>0.002396</td>\n      <td>0.005898</td>\n      <td>0.002035</td>\n      <td>...</td>\n      <td>0.002038</td>\n      <td>0.001974</td>\n      <td>0.002214</td>\n      <td>0.003002</td>\n      <td>0.002947</td>\n      <td>0.002033</td>\n      <td>0.002294</td>\n      <td>0.002079</td>\n      <td>0.001879</td>\n      <td>0.002083</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>id_001897cda</td>\n      <td>0.002041</td>\n      <td>0.002052</td>\n      <td>0.002055</td>\n      <td>0.003816</td>\n      <td>0.004644</td>\n      <td>0.003374</td>\n      <td>0.002030</td>\n      <td>0.005303</td>\n      <td>0.002035</td>\n      <td>...</td>\n      <td>0.002038</td>\n      <td>0.002162</td>\n      <td>0.002117</td>\n      <td>0.001061</td>\n      <td>0.005006</td>\n      <td>0.002033</td>\n      <td>0.005964</td>\n      <td>0.002051</td>\n      <td>0.002421</td>\n      <td>0.002085</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>id_002429b5b</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>id_00276f245</td>\n      <td>0.002041</td>\n      <td>0.002052</td>\n      <td>0.002055</td>\n      <td>0.010955</td>\n      <td>0.008548</td>\n      <td>0.003237</td>\n      <td>0.002358</td>\n      <td>0.004069</td>\n      <td>0.002035</td>\n      <td>...</td>\n      <td>0.002038</td>\n      <td>0.002048</td>\n      <td>0.002364</td>\n      <td>0.005247</td>\n      <td>0.002792</td>\n      <td>0.002033</td>\n      <td>0.002569</td>\n      <td>0.002070</td>\n      <td>0.002198</td>\n      <td>0.002063</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>id_0027f1083</td>\n      <td>0.002041</td>\n      <td>0.002052</td>\n      <td>0.002055</td>\n      <td>0.014007</td>\n      <td>0.017502</td>\n      <td>0.002726</td>\n      <td>0.003422</td>\n      <td>0.004535</td>\n      <td>0.002035</td>\n      <td>...</td>\n      <td>0.002038</td>\n      <td>0.001964</td>\n      <td>0.002480</td>\n      <td>0.005332</td>\n      <td>0.002750</td>\n      <td>0.002033</td>\n      <td>0.002204</td>\n      <td>0.002079</td>\n      <td>0.001766</td>\n      <td>0.002074</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 207 columns</p>\n</div>"},"metadata":{}}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}